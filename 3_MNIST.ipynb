{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting for lesson 2\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The torch.nn.functional area specifically gives us access to some handy functions that we might not want to write ourselves\n",
    "import torch.nn as nn\n",
    "#always pass parameters\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#buildig the class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #runnig the initialization \n",
    "        #creating four layers\n",
    "        self.fc1 = nn.Linear(28*28, 64) #fc: fully connected, passing the falttened image, output is 64                                                  \n",
    "        self.fc2 = nn.Linear(64, 64) #inout is 64 from previous layer and outputs a 64 \n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10) # the last output has 10 classes\n",
    "        \n",
    "    #feedforward data\n",
    "    def forward(self, x): #passing x through all the layers\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         x = self.fc4(x)\n",
    "#         return x #\n",
    "        # rectified linear, or relu, activation function. runs on the output\n",
    "        #to fire the neuron\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) #only passing through\n",
    "        \n",
    "        #probability distribution on the final layer, the sum will be 1.        \n",
    "        #dim=1 is as the outut is 1-d \n",
    "        return F.log_softmax(x, dim=1) \n",
    "    \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0078,  0.0482, -0.8916, -0.3957,  0.4930, -0.7820, -0.7155, -0.4845,\n",
       "          0.5998, -1.4338,  0.2264, -1.4474,  0.2748,  0.1778, -0.3453,  0.7963,\n",
       "          0.4202, -1.5508,  1.5715, -0.6190, -1.7879, -3.1897, -0.0172, -0.0849,\n",
       "         -1.1175,  0.9958, -1.2913, -1.3790],\n",
       "        [-1.3709,  1.7348,  1.2351, -0.3273,  0.4056, -1.5921,  0.2675,  0.3050,\n",
       "         -0.1461, -0.8402, -1.0324, -0.4228, -0.7385, -0.1373,  2.3620, -0.7878,\n",
       "          1.0072, -0.8244,  0.2670, -1.0880, -0.3668,  0.9619,  0.3920, -0.6821,\n",
       "          0.6075, -0.2814, -0.5039, -0.5733],\n",
       "        [ 0.2040,  1.5582,  0.1739, -0.9837, -0.7723, -0.1652, -1.8900,  0.7923,\n",
       "         -0.7715, -1.3415, -0.9316,  1.4467, -0.1679,  2.4510,  0.2840,  1.2731,\n",
       "          0.7858,  0.2834,  0.8064, -0.2876,  0.9722,  0.9708,  0.8163, -0.2423,\n",
       "          0.3302, -0.9795,  0.1460, -0.1567],\n",
       "        [ 0.2256, -0.0404,  0.6193, -0.3355, -0.7262,  0.2327, -0.5786,  1.1355,\n",
       "          0.2098,  0.8508,  0.4430,  2.0527, -0.2430, -0.1368, -1.6864,  0.9008,\n",
       "         -0.9429, -0.5144,  0.5337, -0.2271, -3.4160,  0.3390, -2.3521, -0.0396,\n",
       "          1.0633, -1.4055, -0.4492,  0.5423],\n",
       "        [ 0.9702,  0.2417,  0.3309,  0.6361, -0.8861, -0.4561,  0.2837, -0.9113,\n",
       "         -0.1318,  0.2188,  1.1702, -0.8586, -0.6144,  0.2620, -1.9794, -1.8407,\n",
       "          1.0452, -0.5574,  0.3393, -1.3792, -0.0525,  0.2187, -2.3737,  1.3044,\n",
       "          0.7949,  1.0218,  1.0748,  1.0847],\n",
       "        [ 2.7500, -0.2783, -0.8086,  0.3518, -0.2170,  0.7787, -0.2778, -2.9067,\n",
       "          1.2311,  0.1576, -0.3478, -2.5238, -1.5465, -0.4469, -0.6815, -0.8921,\n",
       "         -0.5314, -0.1085, -1.3358, -1.6188, -0.4647,  1.2387,  0.3477,  0.4534,\n",
       "          1.1692,  1.0541,  0.6204, -1.3162],\n",
       "        [-0.7345, -0.0452,  0.5610,  0.9140, -0.0549,  0.2988,  0.1280,  0.1076,\n",
       "         -0.7081,  1.2927,  0.4916,  0.3030, -1.1068,  0.3572,  0.5650,  0.3263,\n",
       "         -0.4269, -1.3432, -0.7247,  1.5270, -1.3308, -0.1894,  1.0906,  0.2058,\n",
       "          1.3297,  1.0290, -0.0690,  0.1373],\n",
       "        [-0.1406,  0.0350, -0.0789, -0.3021, -0.6252,  0.6164, -0.0131, -0.5647,\n",
       "         -0.3918,  0.0570, -0.4524, -0.3425,  0.3586, -0.2506, -0.9110, -1.3585,\n",
       "          0.9880, -0.3545,  0.9789,  0.4731,  0.9244,  0.4038,  0.7440, -0.3658,\n",
       "         -0.5121, -1.4260,  0.6534, -0.4871],\n",
       "        [-1.5552, -1.7115, -0.3974, -0.6599,  0.4574,  1.4821,  1.3411, -1.2783,\n",
       "          0.0624, -0.3828, -0.3668, -0.5283, -1.0047, -1.8059,  0.5191, -0.2140,\n",
       "         -1.2575,  0.7493, -1.3937, -0.5088,  0.4876, -0.5991, -0.7412, -0.2811,\n",
       "          0.0391, -0.1764,  0.1880,  1.0939],\n",
       "        [-0.0723, -1.6937,  0.1225,  0.2601,  0.8649,  0.3486,  0.2685, -0.0689,\n",
       "         -0.7466, -0.7691, -0.3639, -0.6689,  0.2656, -1.5670, -0.8996, -0.4052,\n",
       "         -0.0570,  0.6006,  0.0656,  1.4452, -0.8641,  1.3229,  0.1038,  0.4728,\n",
       "         -1.3942,  0.2860, -0.4713, -0.4617],\n",
       "        [-1.1418,  0.4151, -1.1955,  0.4798, -0.7492, -3.2093,  0.8282,  0.0167,\n",
       "          1.0144, -0.2284,  1.3199, -1.6070, -0.4181, -0.4242, -2.2345, -0.2932,\n",
       "          1.3789,  0.2070, -0.2026,  0.0911,  0.2885, -0.3793,  0.6470, -0.0803,\n",
       "          0.4643,  1.4713, -1.7817, -1.7413],\n",
       "        [ 0.1710, -1.9448,  0.9526, -0.2912, -0.7429,  1.2145, -0.2368, -0.3763,\n",
       "          1.0841,  0.5741,  1.5820, -0.0518,  1.2047,  0.4093,  0.3444,  0.0663,\n",
       "         -0.4985,  1.2217,  0.2988, -1.9063, -1.0510,  0.8594, -0.5931, -0.2791,\n",
       "          0.0138, -0.7407,  0.3408, -0.2436],\n",
       "        [-1.2367,  0.8757,  2.2379, -0.3172,  0.6546, -0.3064,  1.0507, -0.5675,\n",
       "          0.2520, -0.2866,  0.9715,  0.3737, -0.5566, -1.0395, -0.7502, -1.2668,\n",
       "          1.2871,  0.6959,  1.6801, -0.8373, -0.7557,  0.2075, -0.6584, -0.5477,\n",
       "          1.1681,  1.7915,  0.1007,  1.3180],\n",
       "        [-0.3102,  2.4234,  0.1804, -1.3096,  0.8262,  1.0488,  0.9607, -1.7155,\n",
       "         -0.1666,  1.2646,  0.4089,  0.5487, -0.2223,  0.5459, -0.7075, -0.1071,\n",
       "          1.2145, -1.8992, -0.0532,  0.6080, -0.2612,  0.8495, -0.2220, -0.6850,\n",
       "         -1.0170, -0.0436,  0.7454, -0.1380],\n",
       "        [ 0.4311, -1.3603, -0.2088,  0.9259, -0.3863,  0.0179, -0.5551,  1.4766,\n",
       "          0.9917, -1.1271, -0.3158, -1.0388,  0.4781,  2.1122, -0.8832, -1.7598,\n",
       "          0.1780, -0.6427, -1.2018, -0.7200, -0.2180, -0.5703, -1.0157, -0.4830,\n",
       "          0.5352, -0.6140,  0.0060,  0.5456],\n",
       "        [-0.0519, -0.8799,  0.1625, -0.6046,  0.8839, -0.3723, -0.1709,  0.9092,\n",
       "          1.1856, -0.3195,  0.1521,  1.5551, -2.3397, -0.1402, -0.4799,  0.9708,\n",
       "          0.8162,  0.0585, -0.0781,  0.0573,  0.9028,  0.1003, -0.2006,  0.6729,\n",
       "         -1.1140,  1.0336, -1.2630, -1.1581],\n",
       "        [-0.7538, -0.5777,  0.9360, -0.9611,  0.2739,  0.7647, -0.3456,  0.5216,\n",
       "         -0.3286, -1.8808,  0.0863, -0.6213, -0.4787,  0.1507, -1.0405, -0.6131,\n",
       "         -0.5430,  0.1677,  0.2434,  1.2272, -0.9286, -1.2118, -0.7667, -1.2621,\n",
       "          0.1707, -1.3689,  0.0657,  0.2352],\n",
       "        [ 1.0760, -1.3668, -1.6811,  0.5728,  1.3528, -0.6889,  1.2409,  0.0371,\n",
       "         -0.1952,  0.6240, -0.1573, -0.3848, -1.7163, -1.4623, -0.3423,  0.2526,\n",
       "         -0.1608, -1.1806,  1.1020,  1.0008,  1.0062, -0.2920, -1.3988,  1.4424,\n",
       "         -0.1557,  0.9789,  0.3171, -0.0264],\n",
       "        [ 0.8282, -1.3193, -0.2688, -1.0089,  1.6084, -0.3632,  0.7994, -0.7449,\n",
       "         -1.3221, -0.0912, -0.0592, -0.3205,  1.3385, -0.3243,  0.4104,  1.3893,\n",
       "         -1.2120, -0.9389,  0.3993,  0.5615, -0.0978, -1.3071, -0.8593, -1.0092,\n",
       "         -1.2996,  0.8817,  0.1648, -0.9292],\n",
       "        [ 1.7862, -0.0781,  0.5839,  0.1936,  1.0307,  0.0805,  0.2858,  0.6117,\n",
       "         -0.6963,  1.6670,  1.6466,  0.3655,  0.2954, -0.2525, -0.9754, -0.2664,\n",
       "          0.2224, -1.3660, -1.1102,  0.5957,  0.9410, -1.3778,  0.2921,  0.5295,\n",
       "          0.6554,  0.5986,  0.6515, -0.9812],\n",
       "        [ 1.4386, -0.4583, -1.0116, -0.6426,  1.2535, -0.0632,  0.2166,  0.4384,\n",
       "         -1.0392,  1.1515,  0.3819, -0.0657, -0.9832, -0.7189, -0.3296, -0.0783,\n",
       "         -0.8080,  0.5772,  0.1545, -0.7981, -0.1608, -0.7188,  1.0208, -0.3529,\n",
       "          0.8100, -1.1539, -1.9339,  0.0434],\n",
       "        [ 0.6025,  0.4682,  0.0561, -0.2949, -0.6116,  0.3828, -0.2548,  1.4666,\n",
       "          1.9852,  0.5208, -0.7182,  2.4200,  1.2301, -1.1987,  0.8148, -1.3437,\n",
       "          0.1691,  1.7467,  0.6584,  0.7671,  0.1120,  0.1225, -0.7471,  0.8063,\n",
       "          0.5879, -1.5179, -1.1733, -0.7288],\n",
       "        [ 1.0471, -1.2164, -0.2613, -0.3920, -0.1628,  0.3050, -2.1274,  1.3468,\n",
       "          0.2572,  0.7602,  0.7956,  0.6068, -0.9343,  1.0632,  0.0353, -0.7151,\n",
       "         -0.8344, -0.5572,  1.4311,  0.7870,  0.4035, -0.1046, -2.2032, -0.0382,\n",
       "          1.3887,  0.5741,  0.8436,  1.0249],\n",
       "        [-0.3359,  0.3335, -0.9486,  0.8084,  0.7566, -0.1565,  0.0372, -0.6038,\n",
       "         -1.1482, -0.7491,  0.6742, -0.3407, -1.2058, -0.6916, -0.1840, -0.9128,\n",
       "         -1.4176,  1.1232,  1.5629,  0.2329, -0.0403, -0.0095, -0.0894,  0.7937,\n",
       "          0.6488,  0.9777,  1.4255, -0.0287],\n",
       "        [ 0.0451, -0.7967,  0.0487, -0.7852, -0.2892, -1.0564, -1.3678,  1.6202,\n",
       "         -0.1035,  0.0927, -0.0328, -0.4331, -2.3201,  0.3766, -0.3239, -0.4303,\n",
       "         -0.9036, -0.2096, -0.7158, -0.0642, -0.0212, -1.1430,  0.3486,  0.6540,\n",
       "         -1.6795,  1.1344,  1.1868,  0.4887],\n",
       "        [-0.2653,  1.6555, -2.2067, -1.4385,  1.5231, -0.5826,  0.1771, -0.3539,\n",
       "          0.0214, -0.1912,  1.6384, -1.6261,  0.2554, -1.1345, -0.5084,  0.1082,\n",
       "         -0.6398,  0.1444, -0.6792, -1.5019, -0.0195,  0.3190, -1.6657,  0.2945,\n",
       "          0.6822, -0.7970,  0.3752,  0.3804],\n",
       "        [-0.1880, -0.7458,  0.7867, -0.3964, -0.7569,  0.5382, -0.0535,  0.3432,\n",
       "         -1.2348, -0.0533,  1.0531, -0.5800, -1.9788,  0.4380,  1.3347, -0.7090,\n",
       "          0.2452, -1.0807, -0.7368,  0.2624,  1.2324, -0.5076, -0.6468, -0.7658,\n",
       "         -0.2033,  1.2946,  0.5203,  1.8706],\n",
       "        [ 1.0632, -1.4051,  0.8344, -0.0948, -0.8709, -0.8866, -1.6835, -1.2160,\n",
       "         -0.1222,  0.2288, -2.7393, -0.5600,  0.9642, -0.6006,  1.3722, -0.0179,\n",
       "         -0.2205, -1.0313, -0.5784,  1.2291,  0.9995,  0.9889,  0.9531, -0.9190,\n",
       "         -0.0053, -0.0663, -0.0847,  1.1211]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sending some data\n",
    "X = torch.randn((28,28))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1931, -2.3601, -2.2421, -2.4054, -2.3651, -2.2545, -2.2849, -2.2620,\n",
       "         -2.3471, -2.3318]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flattenign the data, -1 is to say it can be of any size\n",
    "X = X.view(-1,28*28)\n",
    "output = net(X) #passing the data through the network\n",
    "output #grand_fn is gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0018, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#lesson 4 starts here -------------------------------------------\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#loss is how wrong we were\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#the first parameter to change everything\n",
    "#Adam=Adaptive Momentum\n",
    "#the second parameter is learning rate. how big of a step to take\n",
    "#The learning rate dictates the magnitude of changes that the optimizer can make at a time\n",
    "#decaying learing rate coudl be used, but here it is constant\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3 # 3 full passes over the data\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.972\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#showing the xth element image\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()\n",
    "plt.imshow(X[2].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "#number in the xth element\n",
    "print(torch.argmax(net(X[0].view(-1,784))[0]))\n",
    "print(torch.argmax(net(X[2].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3504e+01, -8.7283e+00, -1.3496e+01, -1.1738e+01, -1.9557e+01,\n",
      "        -1.7723e+01, -3.4892e+01, -1.8273e-04, -1.7849e+01, -1.1388e+01],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "a_featureset = X[0]\n",
    "reshaped_for_network = a_featureset.view(-1,784) # 784 b/c 28*28 image resolution.\n",
    "output = net(reshaped_for_network) #output will be a list of network predictions.\n",
    "first_pred = output[0]\n",
    "print(first_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "biggest_index = torch.argmax(first_pred)\n",
    "print(biggest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
